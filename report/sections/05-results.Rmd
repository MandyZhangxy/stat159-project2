---
output: pdf_document
---

#Results

```{r, echo=FALSE}
load("../../data/lasso_model.RData")
load("../../data/OLS.RData")
load("../../data/all_ridge_mods.RData")
load("../../data/pcr_fit.RData")
load("../../data/plsr_model.RData")
library(xtable)
library(knitr)

```

##Ridge
We perform ridge regresssion on the centered Credit training data, and obtain the following result:
The best lambda equals `r best_lambda`, and the test mse is `r test_mse`.
The coefficient of fitting the full data is following:

```{r xtable1, results = 'asis', echo = FALSE, warning=FALSE, comment= FALSE, message=FALSE}

ridgecoef <- xtable(as.matrix(ridge_coef))
print(ridgecoef,comment = FALSE)
```

##Lasso
Then we fit lasso regression on the centered Credit training data.
The best lambda is `r best_lambda`, and the lasso test MSE is `r lasso_mse`.
The refitting coefficients is the following:
```{r xtable2, results = 'asis', echo = FALSE, warning=FALSE, comment= FALSE, message=FALSE}
load("../../data/lasso_model.RData")
lassocoef <- xtable(as.matrix(lasso_coef))
print(lassocoef,comment = FALSE)
```
We observe some coefficients could reduce to zero because of the special regularizing term the lasso regression has.


##PCR
Now we use a different method, the principle component methods to fit on the training data. In this case, we think the optimal number of principle components used is `r pcr_best`, and the resulting test MSE is `r pcr_mse`.
The coefficients of PCR model refitting on full data set is:
```{r xtable3, results = 'asis', echo = FALSE, warning=FALSE, comment= FALSE, message=FALSE}
load("../../data/pcr_fit.RData")
pcrcoef <- xtable(as.matrix(pcr.coef))
print(pcrcoef,comment = FALSE)
```

##PLSR
We slightly change our method to PLSR. The optimal number of principle components is `r plsr_best`, and the resulting test MSE is `r plsr_mse`
The coefficient of refitting PLSR model on full dataset is:
```{r xtable4, results = 'asis', echo = FALSE, warning=FALSE, comment= FALSE, message=FALSE}
load("../../data/plsr_model.RData")
plsrcoef <- xtable(as.matrix(plsr_coef))
print(plsrcoef,comment = FALSE)
```

#OLS
At last we look at the ordinary least square regression:
The coefficients of the model that includes all predictors is:
```{r xtable5, results = 'asis', echo = FALSE, warning=FALSE, comment= FALSE, message=FALSE}
load("../../data/OLS.RData")
olscoef <- xtable(OLS_summary$coefficients, caption = "OLS coefficients")
print(olscoef,comment = FALSE)
```
The R-square is `r OLS_summary$r.squared`. The Residual Standard Error is `r OLS_summary$sigma`




Below is the table for OLS regression summary. As we can see, certain coefficients comes with a relatively high p-value, like education and ethnicity which suggests that they may not be significant. Also, if we look at the absolute value of the estimated coefficients, we can see that income, limit and ratins dominate the change in reponse variable, which suggests that we should try principal components regression that will ignore trivial variables.

{r results= 'asis', echo = FALSE}
library(xtable)
library(Matrix)
options(xtable.comment = FALSE,
        xtable.table.placement = "H")
load("../data/OLS/OLS-regression.RData")
print(xtable(ols_reg_sum$coefficients, caption = 'OLS Coefficients',digits = c(0,5,5,5,5)), comment = FALSE)
```

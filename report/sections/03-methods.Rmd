#Methods

------

## [Ordinary Least Square Method](https://en.wikipedia.org/wiki/Ordinary_least_squares)
In statistics, ordinary least squares (OLS) or linear least squares is a method for estimating the unknown parameters in a linear regression model, with the goal of minimizing the sum of the squares of the differences between the observed responses in the given dataset and those predicted by a linear function of a set of explanatory variables (visually this is seen as the sum of the vertical distances between each data point in the set and the corresponding point on the regression line â€“ the smaller the differences, the better the model fits the data). 
$$RSS = \sum_{i=1}^n\left(y_i - \hat{\beta_0} - \sum_{j=1}^p(\hat{\beta_j}x_{ij})\right)^2$$
or equivalently as 
$$RSS = (y_1 - \hat{\beta_0} - \sum_{j=1}^p(\hat{\beta_j}x_{1j}))^2 + (y_2 - \hat{\beta_0} - \sum_{j=1}^p(\hat{\beta_j}x_{2j}))^2 + ... + (y_{n} - \hat{\beta_0} - \sum_{j=1}^p(\hat{\beta_j}x_{nj}))^2$$

## Shrinkage methods
This approach involves fitting a model involving all `p` predictors. However, the estimated coefficients are shrunken towards zero relative to the least squares estimates. This shrinkage (also known as *regularization*) has the effect of reducing variance. Depending on what type of shrinkage is performed, some of the coefficients may be estimated to be exactly zero. Hence, shrinkage methods can also perform variable selection. It may not be immediately obvious why such a constraint should improve the fit, but it turns out that shrinking the coefficient estimates can significantly reduce their variance. The two best-known techniques for shrinking the regression coefficients towards zero are *ridge regression* the the  *lasso*.

### Ridge Regression
*Ridge regression* is very similar to least squares, except that the coefficients are estimated by minimizing a slightly different quantity. In particular, the ridge regression coefficinet estiamtes $\hat{\beta}^R$ are the values that minimize 

$$\sum_{i=1}^n{\left(y_i-\beta_0 - \sum_{j=1}^p{\beta_jx_{ij}}\right)^2} + \lambda\sum_{j=1}^p{\beta_j^2} = RSS + \lambda\sum_{j=1}^p{\beta_j^2}$$ 

where $\lambda \geq 0$ is a *tunig parameter*, to be determined separately. Equation trades off two different criteria. as with least squares, ridge regression seeks coefficinet estimates that fit the data well, by making the RSS small. however, the second term, $\lambda\sum_{j=1}^p{\beta_j^2}$, called a *shrinkage penalty*, is small when $\beta_1,...,\beta_p$ are close to zero, and so it has the effect of *shrinking* the estimates of $\beta_j$ towards zero. The tunning parameter $\lambda$ serves to control the relative impact of these two terms on the regression coefficient estiamtes. 

### Lasso Regression


## Dimension Reduction Method
This approach involves *prejecting* the `p` predictors into a *M*-dimensional subsapce, where M < p. This is achieved by computing *M* different *linear combinations*, or *projections*, of the variables. Then these *M* projections are used as predictors to fit a linear regression model by least squares. 

### Principal Components Regression (PCR)

### Partial Least Square Regression (PLSR)
